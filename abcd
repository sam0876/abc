#Take a quick peek:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import base64
import string
import re
from collections import Counter
from nltk.corpus import stopwords
stopwords = stopwords.words('english')
import re
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer 
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import accuracy_score,confusion_matrix,precision_score,recall_score 
from nltk.stem.porter import PorterStemmer
from nltk import word_tokenize   
from sklearn.naive_bayes import MultinomialNB   
from sklearn.model_selection import train_test_split
from sklearn import svm 
import random
import numpy as np 
from sklearn import preprocessing 
from nltk.corpus import wordnet
df = pd.read_excel('D:\\ACH\\Ach_data_201812_deposit.xlsx', names= ('number','text'))
df.head(10)
df.head(50)
from wordcloud import WordCloud, STOPWORDS 
df.isnull().sum()
df.describe()
new= df['text'].str.split("-", n=1, expand= True)
# making separate first name column from new data frame 
new["Text"]= new[0] 

# making separate last name column from new data frame 
new["Type"]= new[1]
# Dropping old Name columns 
new.drop(columns =[0], inplace = True) 
# Dropping old Name columns 
new.drop(columns =[1], inplace = True) 

comment_words = ' '
stopwords = set(STOPWORDS) 

reader_contents = list(new) 

from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator

import matplotlib.pyplot as plt

print("There are {} observations and {} features in this dataset. \n".format(new.shape[0],new.shape[1]))

print("There are {} types of variables type in this dataset such as {}... \n".format(len(new.Type.unique()),
                                                                           ", ".join(new.Type.unique()[0:5])))

                                                                           In [51]:


new[["Type", "Text"]].head()
# Groupby by country
Type = new.groupby("Type")

# Summary statistic of all countries
Type.describe().head()

plt.figure(figsize=(15,10))
Type.size().sort_values(ascending=False).plot.bar()
plt.xticks(rotation=50)
plt.xlabel("Type")
plt.ylabel("Text")
plt.show()

type(new)

new["Text"] = new["Text"].map(lambda x: x.lower())
#print(xLower_Text)
#print(type(xLower_Text))
#xLower_Text.head(20)


new["Type"] = new["Type"].map(lambda x: x.lower())
#print(xLower_Text)
#print(type(xLower_Text))
#xLower_Text.head(20)

# Groupby by country
Type = new.groupby("Type")

# Summary statistic of all countries
Type.describe().head()

new.isnull().sum()

new['wordtype'] = new.Text.str.strip().str.split('[\W_]+')
new.head()


#new['words'].replace(regex=True, inplace=True, to_replace=r'[^0-9.\-]')
#new['words'] = new['words'].replace({';':''}, regex=True)
new['Text'] = new.Text.str.replace('\d+', '')
new['Type'] = new.Type.str.replace('\d+', '')
word= new[['Text', 'Type']]
word.head(40)
#import re
#line = re.sub('[!@#$]', '', word)


word=word.replace('\(;','',regex=True)
word=word.replace('\-','',regex=True)

word.head(40)

# word[]= word['Text'].replace(regex=True, inplace=True, to_replace=r'[^0-9.\-]')
#df['A1'] = df['A1']
word.replace(regex=True, inplace=True, to_replace=r'[^^(?=.*\d)(?=.*[A-Za-z])[A-Za-z0-9]{1,10}$$ ((;;]')

word.head(50)

word.Text=word.Text.str.replace(';','')

word.Text=word.Text.str.replace('(','')

word.Text=word.Text.str.replace('#','')

def lemmatize(word):
    lemmatizer = nltk.stem.WordNetLemmatizer()
    lemmatized = word['tokenized'].apply(
            lambda row: list(list(map(lemmatizer.lemmatize,y)) for y in row))
    return lemmatized


# new data frame with split value columns 
new = word["Type"].str.split(" ", n = 3, expand = True) 


# making separate first name column from new data frame 
word["Type1"]= new[1] 
word["Type2"]= new[2] 
# Dropping old Name columns 
word.drop(columns =["Type"], inplace = True) 

# df display 
word 



# making separate last name column from new data frame 
word["Type2"]= new[2] 


# Dropping old Name columns 
word.drop(columns =["Type"], inplace = True) 

# df display 
word 


# replace the matching strings 
word.Type1 = word.Type1.replace(to_replace ='bill.com', value = 'online', regex = True)
word.Type1 = word.Type1.replace(to_replace ='payrll', value = 'payroll', regex = True)  

word.Type1 = word.Type1.replace(to_replace ='pp', value = 'p2p', regex = True) 

# Groupby by country
Type = word.groupby("Type1")

# Summary statistic of all countries
Type.describe().head()

plt.figure(figsize=(15,10))
Type.size().sort_values(ascending=False).plot.bar()
plt.xticks(rotation=50)
plt.xlabel("Type1")
plt.ylabel("Text")
plt.show()


from fuzzywuzzy import process
input_dictionary= {"payment", "salary", "credit", "transaction","transfer","deposit","payroll", "p2p"}

def replacement_function(input_string, input_dictionary, threshold):

    best_guess = process.extractOne(input_string, input_dictionary)
    if best_guess[1] > threshold:
         input_string = best_guess[0]
         #print(best_guess)
    return input_string  

word['Type1'] = word['Type1'].apply(lambda x: ' '.join([replacement_function(word,input_dictionary,75) for word in x.split()]))

#################
#df2 = df['Text']


from collections import Counter
a=Counter(" ".join(word["Type1"]).split()).most_common(11)

from collections import Counter
Counter(" ".join(word["Type1"]).split()).most_common(11)

l = []
for i in a:
    l.append(i[0])



new=word.loc[word['Type1'].isin(l)]



# Groupby by country
Type1 = new.groupby("Type1")

# Summary statistic of all countries
Type1.describe().head()














































plt.figure(figsize=(15,10))
Type1.size().sort_values(ascending=False).plot.bar()
plt.xticks(rotation=50)
plt.xlabel("Type1")
plt.ylabel("Text")
plt.show()


[82]:
 
 
new.isnull().sum()


t_nlp= new

t_nlp.head(60)

t_nlp.Text.replace(regex=True, inplace=True, to_replace=r'[^^(?=.*\d$&)(?=.*[A-Za-z])[A-Za-z0-9]{1,10}$$ ((;;].$& //')

t_nlp.head(50)


t_nlp.Text=t_nlp.Text.str.replace('.','')

# new data frame with split value columns 
new_t = t_nlp["Text"].str.split(" ", n = 3, expand = True)

# making separate first name column from new data frame 
word["Type1"]= new[1] 
word["Type2"]= new[2] 
# Dropping old Name columns 
word.drop(columns =["Type"], inplace = True) 
  
# df display 
Word

from sklearn.model_selection import train_test_split
train, test = train_test_split(t_nlp, test_size=0.33, random_state=42)

print('Type of Text:', train['Text'].iloc[0])
print('Type of Ach:', train['Type1'].iloc[0])
print('Training Data Shape:', train.shape)
print('Testing Data Shape:', test.shape)


import spacy
nlp = spacy.load('en_core_web_sm')
punctuations = string.punctuation
def cleanup_text(docs, logging=False):
    texts = []
    counter = 1
    for doc in docs:
        if counter % 1000 == 0 and logging:
            print("Processed %d out of %d documents." % (counter, len(docs)))
        counter += 1
        doc = nlp(doc, disable=['parser', 'ner'])
        tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != ' ']
        tokens = [tok for tok in tokens if tok not in stopwords and tok not in punctuations]
        tokens = ' '.join(tokens)
        texts.append(tokens)
    return pd.Series(texts)
INFO_text = [text for text in train[train['Type1'] == 'deposit']['Text']]
IS_text = [text for text in train[train['Type1'] == 'payroll']['Text']]
INFO_clean = cleanup_text(INFO_text)
INFO_clean = ' '.join(INFO_clean).split()
IS_clean = cleanup_text(IS_text)
IS_clean = ' '.join(IS_clean).split()
INFO_counts = Counter(INFO_clean)
IS_counts = Counter(IS_clean)
INFO_common_words = [word[0] for word in INFO_counts.most_common(20)]
INFO_common_counts = [word[1] for word in INFO_counts.most_common(20)]
fig = plt.figure(figsize=(18,6))
sns.barplot(x=INFO_common_words, y=INFO_common_counts)
plt.title('Most Common Words used in the deposit')
plt.show()


import spacy
nlp = spacy.load('en_core_web_sm')
punctuations = string.punctuation
def cleanup_text(docs, logging=False):
    texts = []
    counter = 1
    for doc in docs:
        if counter % 1000 == 0 and logging:
            print("Processed %d out of %d documents." % (counter, len(docs)))
        counter += 1
        doc = nlp(doc, disable=['parser', 'ner'])
        tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']
        tokens = [tok for tok in tokens if tok not in stopwords and tok not in punctuations]
        tokens = ' '.join(tokens)
        texts.append(tokens)
    return pd.Series(texts)
IS_text = [text for text in train[train['Type1'] == 'payroll']['Text']]
IS_clean = cleanup_text(IS_text)
IS_clean = ' '.join(IS_clean).split()
IS_counts = Counter(IS_clean)
IS_common_words = [word[0] for word in IS_counts.most_common(20)]
IS_common_counts = [word[1] for word in IS_counts.most_common(20)]
fig = plt.figure(figsize=(18,6))
sns.barplot(x=IS_common_words, y=IS_common_counts)
plt.title('Most Common Words in Payroll')
plt.show()



from sklearn.feature_extraction.text import CountVectorizer
from sklearn.base import TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.svm import LinearSVC
from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS
from sklearn.metrics import accuracy_score
from nltk.corpus import stopwords
import string
import re
import spacy
spacy.load("en_core_web_sm")
from spacy.lang.en import English
parser = English()


STOPLIST = set(stopwords.words('english') + list(ENGLISH_STOP_WORDS))
SYMBOLS = " ".join(string.punctuation).split(" ") + ["-", "...", "”", "”"]
class CleanTextTransformer(TransformerMixin):
   def transform(self, X, **transform_params):
        return [cleanText(text) for text in X]
   def fit(self, X, y=None, **fit_params):
        return self
def get_params(self, deep=True):
        return {}
    
def cleanText(text):
    text = text.strip().replace("\n", " ").replace("\r", " ")
    text = text.lower()
    return text
def tokenizeText(sample):
    tokens = parser(sample)
    lemmas = []
    for tok in tokens:
        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != "-PRON-" else tok.lower_)
    tokens = lemmas
    tokens = [tok for tok in tokens if tok not in STOPLIST]
    tokens = [tok for tok in tokens if tok not in SYMBOLS]


def printNMostInformative(vectorizer, clf, N):
    feature_names = vectorizer.get_feature_names()
    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))
    topClass1 = coefs_with_fns[:N]
    topClass2 = coefs_with_fns[:-(N + 1):-1]
    print("Class 1 best: ")
    for feat in topClass1:
        print(feat)
    print("Class 2 best: ")
    for feat in topClass2:
        print(feat)
vectorizer = CountVectorizer(tokenizer=tokenizeText, ngram_range=(1,1))
clf = LinearSVC()

pipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer), ('clf', clf)])
# data
train1 = train['Text'].tolist()
labelsTrain1 = train['Type1'].tolist()
test1 = test['Text'].tolist()
labelsTest1 = test['Type1'].tolist()
# train
pipe.fit(train1, labelsTrain1)
# test
preds = pipe.predict(test1)
print("accuracy:", accuracy_score(labelsTest1, preds))
print("Top 10 features used to predict: ")

printNMostInformative(vectorizer, clf, 10)
pipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer)])
transform = pipe.fit_transform(train1, labelsTrain1)
vocab = vectorizer.get_feature_names()
for i in range(len(train1)):
    s = ""
    indexIntoVocab = transform.indices[transform.indptr[i]:transform.indptr[i+1]]
    numOccurences = transform.data[transform.indptr[i]:transform.indptr[i+1]]
    for idx, num in zip(indexIntoVocab, numOccurences):
        s += str((vocab[idx], num))

from sklearn import metrics
print(metrics.classification_report(labelsTest1, preds, 
                                    target_names=new['Type1'].unique()))



from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics import adjusted_rand_score
from sklearn.feature_extraction.text import CountVectorizer


vectorizer = TfidfVectorizer(stop_words='english')
X = vectorizer.fit_transform(df2)

print(vectorizer.vocabulary_)

print(X)

true_k = 10
model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)
model.fit(X)



print("Top terms per cluster:")
order_centroids = model.cluster_centers_.argsort()[:, ::-1]
terms = vectorizer.get_feature_names()
for i in range(true_k):
    print("Cluster %d:" % i),
    for ind in order_centroids[i, :5]:
        print(' %s' % terms[ind]),
    print

print("\n")

#################

cluster_map = pd.DataFrame()
cluster_map['data_index'] = df2.index.values
cluster_map['cluster'] = model.labels_

#Analysis of the cluster dataframe

print(cluster_map)
